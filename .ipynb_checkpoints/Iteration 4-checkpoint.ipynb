{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb967452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/11 22:52:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "import findspark\n",
    "# findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "spark = SparkSession.builder.appName('iteration4').getOrCreate()\n",
    "\n",
    "#disabling progress updates\n",
    "import logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8655c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing file \n",
    "df = spark.read.csv('heart_disease_health_indicators_BRFSS2015.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225e3e4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 22\n",
      "Number of rows: 253680\n"
     ]
    }
   ],
   "source": [
    "#getting number of rows and columns\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Number of rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de976cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HeartDiseaseorAttack: string (nullable = true)\n",
      " |-- HighBP: string (nullable = true)\n",
      " |-- HighChol: string (nullable = true)\n",
      " |-- CholCheck: string (nullable = true)\n",
      " |-- BMI: string (nullable = true)\n",
      " |-- Smoker: string (nullable = true)\n",
      " |-- Stroke: string (nullable = true)\n",
      " |-- Diabetes: string (nullable = true)\n",
      " |-- PhysActivity: string (nullable = true)\n",
      " |-- Fruits: string (nullable = true)\n",
      " |-- Veggies: string (nullable = true)\n",
      " |-- HvyAlcoholConsump: string (nullable = true)\n",
      " |-- AnyHealthcare: string (nullable = true)\n",
      " |-- NoDocbcCost: string (nullable = true)\n",
      " |-- GenHlth: string (nullable = true)\n",
      " |-- MentHlth: string (nullable = true)\n",
      " |-- PhysHlth: string (nullable = true)\n",
      " |-- DiffWalk: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- Income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe36f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null count in 'HeartDiseaseorAttack': 0\n",
      "Null count in 'HighBP': 0\n",
      "Null count in 'HighChol': 0\n",
      "Null count in 'CholCheck': 0\n",
      "Null count in 'BMI': 0\n",
      "Null count in 'Smoker': 0\n",
      "Null count in 'Stroke': 0\n",
      "Null count in 'Diabetes': 0\n",
      "Null count in 'PhysActivity': 0\n",
      "Null count in 'Fruits': 0\n",
      "Null count in 'Veggies': 0\n",
      "Null count in 'HvyAlcoholConsump': 0\n",
      "Null count in 'AnyHealthcare': 0\n",
      "Null count in 'NoDocbcCost': 0\n",
      "Null count in 'GenHlth': 0\n",
      "Null count in 'MentHlth': 0\n",
      "Null count in 'PhysHlth': 0\n",
      "Null count in 'DiffWalk': 0\n",
      "Null count in 'Sex': 0\n",
      "Null count in 'Age': 0\n",
      "Null count in 'Education': 0\n",
      "Null count in 'Income': 0\n"
     ]
    }
   ],
   "source": [
    "#checking data for null values\n",
    "null_counts = [df.select(sum(col(c).isNull().cast(\"int\")).alias(c)).collect()[0][0] for c in df.columns]\n",
    "\n",
    "for col_name, null_count in zip(df.columns, null_counts):\n",
    "    print(f\"Null count in '{col_name}': {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f222d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDiseaseorAttack: ['0', '1']\n",
      "HighBP: ['0', '1']\n",
      "HighChol: ['0', '1']\n",
      "CholCheck: ['0', '1']\n",
      "BMI: ['51', '54', '15', '29', '69', '42', '73', '87', '64', '30', '34', '59', '28', '22', '85', '35', '16', '52', '71', '98', '47', '96', '43', '31', '18', '70', '27', '61', '75', '17', '26', '46', '77', '89', '60', '68', '90', '19', '23', '41', '55', '95', '40', '38', '25', '44', '82', '53', '92', '86', '58', '81', '33', '48', '67', '84', '79', '24', '32', '88', '20', '56', '36', '37', '49', '63', '65', '39', '62', '12', '83', '13', '21', '14', '66', '91', '74', '72', '76', '80', '50', '45', '57', '78']\n",
      "Smoker: ['0', '1']\n",
      "Stroke: ['0', '1']\n",
      "Diabetes: ['0', '1', '2']\n",
      "PhysActivity: ['0', '1']\n",
      "Fruits: ['0', '1']\n",
      "Veggies: ['0', '1']\n",
      "HvyAlcoholConsump: ['0', '1']\n",
      "AnyHealthcare: ['0', '1']\n",
      "NoDocbcCost: ['0', '1']\n",
      "GenHlth: ['3', '5', '1', '4', '2']\n",
      "MentHlth: ['7', '15', '11', '29', '3', '30', '8', '28', '22', '16', '0', '5', '18', '27', '17', '26', '6', '19', '23', '25', '24', '9', '1', '20', '10', '4', '12', '13', '21', '14', '2']\n",
      "PhysHlth: ['7', '15', '11', '29', '3', '30', '8', '28', '22', '16', '0', '5', '18', '27', '17', '26', '6', '19', '23', '25', '24', '9', '1', '20', '10', '4', '12', '13', '14', '21', '2']\n",
      "DiffWalk: ['0', '1']\n",
      "Sex: ['0', '1']\n",
      "Age: ['7', '11', '3', '8', '5', '6', '9', '1', '10', '4', '12', '13', '2']\n",
      "Education: ['3', '5', '6', '1', '4', '2']\n",
      "Income: ['7', '3', '8', '5', '6', '1', '4', '2']\n"
     ]
    }
   ],
   "source": [
    "#checking unique values\n",
    "unique_values = {}\n",
    "\n",
    "for col_name in df.columns:\n",
    "    values = [str(row[col_name]) for row in df.select(col_name).distinct().collect()]\n",
    "    unique_values[col_name] = values\n",
    "\n",
    "for column, values in unique_values.items():\n",
    "    print(f\"{column}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e7db91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distribution chart of heart disease    \n",
    "category_counts = df.groupBy('HeartDiseaseorAttack').count().toPandas()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the bar chart\n",
    "colors = ['green', 'darkred']\n",
    "plt.bar(category_counts['HeartDiseaseorAttack'].astype(str), category_counts['count'], color=colors)\n",
    "plt.xlabel('Heart Disease')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar Chart of Heart Disease')\n",
    "plt.xticks(category_counts['HeartDiseaseorAttack'].astype(str), ['0', '1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e184e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! 2 bar charts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d8632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HeartDiseaseorAttack: string (nullable = true)\n",
      " |-- HighBP: string (nullable = true)\n",
      " |-- HighChol: string (nullable = true)\n",
      " |-- CholCheck: string (nullable = true)\n",
      " |-- BMI: string (nullable = true)\n",
      " |-- Smoker: string (nullable = true)\n",
      " |-- Stroke: string (nullable = true)\n",
      " |-- Diabetes: string (nullable = true)\n",
      " |-- PhysActivity: string (nullable = true)\n",
      " |-- Fruits: string (nullable = true)\n",
      " |-- Veggies: string (nullable = true)\n",
      " |-- HvyAlcoholConsump: string (nullable = true)\n",
      " |-- AnyHealthcare: string (nullable = true)\n",
      " |-- NoDocbcCost: string (nullable = true)\n",
      " |-- GenHlth: string (nullable = true)\n",
      " |-- MentHlth: string (nullable = true)\n",
      " |-- PhysHlth: string (nullable = true)\n",
      " |-- DiffWalk: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping columns\n",
    "columns = ['Education', 'Income']  \n",
    "df1 = df.drop(*columns)\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f764966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! bar chart for BMI distribution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "192efb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HeartDiseaseorAttack: string (nullable = true)\n",
      " |-- HighBP: string (nullable = true)\n",
      " |-- HighChol: string (nullable = true)\n",
      " |-- CholCheck: string (nullable = true)\n",
      " |-- BMI: string (nullable = true)\n",
      " |-- Smoker: string (nullable = true)\n",
      " |-- Stroke: string (nullable = true)\n",
      " |-- Diabetes: string (nullable = true)\n",
      " |-- PhysActivity: string (nullable = true)\n",
      " |-- Fruits: string (nullable = true)\n",
      " |-- Veggies: string (nullable = true)\n",
      " |-- HvyAlcoholConsump: string (nullable = true)\n",
      " |-- AnyHealthcare: string (nullable = true)\n",
      " |-- NoDocDueToCost: string (nullable = true)\n",
      " |-- GenHlth: string (nullable = true)\n",
      " |-- MentHlth: string (nullable = true)\n",
      " |-- PhysHlth: string (nullable = true)\n",
      " |-- DifficultyWalking: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renaming columns\n",
    "df2 = df1.withColumnRenamed('Age', 'Age_category') \\\n",
    "          .withColumnRenamed('NoDocbcCost', 'NoDocDueToCost') \\\n",
    "          .withColumnRenamed('DiffWalk', 'DifficultyWalking')\n",
    "\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "453cdbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Diabetes|\n",
      "+--------+\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|  Type 2|\n",
      "|       0|\n",
      "|  Type 2|\n",
      "|       0|\n",
      "|       0|\n",
      "|  Type 2|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|  Type 2|\n",
      "|       0|\n",
      "|       0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replacing values in diabetes column\n",
    "from pyspark.sql.functions import when\n",
    "replacement_mapping = {1: 'Type 1', 2: 'Type 2'}\n",
    "\n",
    "# Replace values in the 'Diabetes' column based on the mapping\n",
    "for key, value in replacement_mapping.items():\n",
    "    df2 = df2.withColumn('Diabetes', when(col('Diabetes') == key, value).otherwise(col('Diabetes')))\n",
    "\n",
    "df2.select('Diabetes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97bb50df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   Sex|\n",
      "+------+\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|  Male|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|  Male|\n",
      "|  Male|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|Female|\n",
      "|  Male|\n",
      "|Female|\n",
      "|  Male|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replacing values in sex column\n",
    "from pyspark.sql.functions import when\n",
    "replacement_mapping = {0: 'Female', 1: 'Male'}\n",
    "\n",
    "# Replace values in the 'Diabetes' column based on the mapping\n",
    "for key, value in replacement_mapping.items():\n",
    "    df2 = df2.withColumn('Sex', when(col('Sex') == key, value).otherwise(col('Sex')))\n",
    "\n",
    "df2.select('Sex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fb95d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|GenHlth|\n",
      "+-------+\n",
      "|   Poor|\n",
      "| Normal|\n",
      "|   Poor|\n",
      "|   Good|\n",
      "|   Good|\n",
      "|   Good|\n",
      "| Normal|\n",
      "| Normal|\n",
      "|   Poor|\n",
      "|   Good|\n",
      "| Normal|\n",
      "| Normal|\n",
      "| Normal|\n",
      "|   Poor|\n",
      "|   Poor|\n",
      "|   Good|\n",
      "| Normal|\n",
      "|   Good|\n",
      "|   Good|\n",
      "|   Good|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#categorizing general health\n",
    "categories = {1: 'Good', 2: 'Good', 3: 'Normal', 4: 'Poor', 5: 'Poor'}\n",
    "\n",
    "# Replace values in the 'GenHlth' column based on the mapping\n",
    "for key, value in categories.items():\n",
    "    df2 = df2.withColumn('GenHlth', when(col('GenHlth') == key, value).otherwise(col('GenHlth')))\n",
    "\n",
    "df2.select('GenHlth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36aeea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|PhysHlth|\n",
      "+--------+\n",
      "|    Good|\n",
      "|    Poor|\n",
      "|  Normal|\n",
      "+--------+\n",
      "\n",
      "+-------+\n",
      "|GenHlth|\n",
      "+-------+\n",
      "|   Good|\n",
      "|   Poor|\n",
      "| Normal|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#categorizing physical and mental health\n",
    "mapping = {\n",
    "    (0, 7): 'Good',\n",
    "    (7, 12): 'Normal',\n",
    "    (12, 31): 'Poor'\n",
    "}\n",
    "\n",
    "# Replacing values in PhysHlth\n",
    "for key, value in mapping.items():\n",
    "    df2 = df2.withColumn('PhysHlth', when((col('PhysHlth') >= key[0]) & (col('PhysHlth') < key[1]), value).otherwise(col('PhysHlth')))\n",
    "\n",
    "# Replacing values in MentHlth\n",
    "for key, value in mapping.items():\n",
    "    df2 = df2.withColumn('MentHlth', when((col('MentHlth') >= key[0]) & (col('MentHlth') < key[1]), value).otherwise(col('MentHlth')))\n",
    "\n",
    "df2.select('PhysHlth').distinct().show()\n",
    "df2.select('GenHlth').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cbf5091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|        BMI|\n",
      "+-----------+\n",
      "| Overweight|\n",
      "|Underweight|\n",
      "|      Obese|\n",
      "|    Healthy|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#categorizing BMI\n",
    "mapping = {\n",
    "    (0, 19): 'Underweight',\n",
    "    (19, 25): 'Healthy',\n",
    "    (25, 30): 'Overweight',\n",
    "    (30, 150): 'Obese'\n",
    "}\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    df2 = df2.withColumn('BMI', when((col('BMI') >= key[0]) & (col('BMI') < key[1]), value).otherwise(col('BMI')))\n",
    "\n",
    "df2.select('BMI').distinct().show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a1fede5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDiseaseorAttack: ['0', '1']\n",
      "HighBP: ['0', '1']\n",
      "HighChol: ['0', '1']\n",
      "CholCheck: ['0', '1']\n",
      "BMI: ['Overweight', 'Underweight', 'Obese', 'Healthy']\n",
      "Smoker: ['0', '1']\n",
      "Stroke: ['0', '1']\n",
      "Diabetes: ['0', 'Type 1', 'Type 2']\n",
      "PhysActivity: ['0', '1']\n",
      "Fruits: ['0', '1']\n",
      "Veggies: ['0', '1']\n",
      "HvyAlcoholConsump: ['0', '1']\n",
      "AnyHealthcare: ['0', '1']\n",
      "NoDocDueToCost: ['0', '1']\n",
      "GenHlth: ['Good', 'Poor', 'Normal']\n",
      "MentHlth: ['Good', 'Poor', 'Normal']\n",
      "PhysHlth: ['Good', 'Poor', 'Normal']\n",
      "DifficultyWalking: ['0', '1']\n",
      "Sex: ['Female', 'Male']\n",
      "Age_category: ['7', '11', '3', '8', '5', '6', '9', '1', '10', '4', '12', '13', '2']\n"
     ]
    }
   ],
   "source": [
    "#checking unique values to confirm changes for all columns\n",
    "unique_values = {}\n",
    "\n",
    "for col_name in df2.columns:\n",
    "    values = [str(row[col_name]) for row in df2.select(col_name).distinct().collect()]\n",
    "    unique_values[col_name] = values\n",
    "\n",
    "for column, values in unique_values.items():\n",
    "    print(f\"{column}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c1bc9",
   "metadata": {},
   "source": [
    "## 3.4 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fcca067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 -\n",
      "Number of columns: 20\n",
      "Number of rows: 126840\n",
      "Part 2 -\n",
      "Number of columns: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 126840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#splitting excel files\n",
    "half_rows = df2.count() // 2\n",
    "\n",
    "part1 = df2.limit(half_rows)\n",
    "part2 = df2.exceptAll(part1)\n",
    "\n",
    "#getting number of rows and columns\n",
    "print(\"Part 1 -\")\n",
    "print(\"Number of columns:\", len(part1.columns))\n",
    "print(\"Number of rows:\", part1.count())\n",
    "\n",
    "#getting number of rows and columns\n",
    "print(\"Part 2 -\")\n",
    "print(\"Number of columns:\", len(part2.columns))\n",
    "print(\"Number of rows:\", part2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa8b9215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#creating the excel files and integrating \n",
    "part1.write.csv('part1.csv', header=True, mode='overwrite')\n",
    "part2.write.csv('part2.csv', header=True, mode='overwrite')\n",
    "\n",
    "part1read = spark.read.csv('part1.csv', header=True, inferSchema=True)\n",
    "part2read = spark.read.csv('part2.csv', header=True, inferSchema=True)\n",
    "\n",
    "df3 = part1read.union(part2read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c3f62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 20\n",
      "Number of rows: 253680\n"
     ]
    }
   ],
   "source": [
    "#getting number of rows and columns\n",
    "print(\"Number of columns:\", len(df3.columns))\n",
    "print(\"Number of rows:\", df3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8b84d",
   "metadata": {},
   "source": [
    "## 3.5 Reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9aab7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 20\n",
      "Number of rows: 253680\n"
     ]
    }
   ],
   "source": [
    "part1.write.mode(\"overwrite\").parquet('part1.parquet')\n",
    "part1new = spark.read.parquet('part1.parquet')\n",
    "\n",
    "df4 = part1new.union(part2read)\n",
    "\n",
    "#getting number of rows and columns\n",
    "print(\"Number of columns:\", len(df4.columns))\n",
    "print(\"Number of rows:\", df4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "557be956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! bar chart of heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21697c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|HeartDiseaseorAttack| count|\n",
      "+--------------------+------+\n",
      "|                   0|229787|\n",
      "|                   1| 23893|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts_heartDis = df4.groupBy('HeartDiseaseorAttack').count()\n",
    "\n",
    "# Show the counts\n",
    "counts_heartDis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbea6ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|HeartDiseaseorAttack|count|\n",
      "+--------------------+-----+\n",
      "|                   0|23893|\n",
      "|                   1|23893|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#balancing dataset\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "grouped_df = df4.groupBy('HeartDiseaseorAttack')\n",
    "min_group_size = grouped_df.count().selectExpr('min(count) as min_count').collect()[0]['min_count']\n",
    "w = Window.partitionBy('HeartDiseaseorAttack').orderBy(F.monotonically_increasing_id())\n",
    "dfNewWithRowId = df4.withColumn('row_id', F.row_number().over(w))\n",
    "dfBalanced = dfNewWithRowId.filter(dfNewWithRowId['row_id'] <= min_group_size).drop('row_id')\n",
    "\n",
    "counts_heartDisBal = dfBalanced.groupBy('HeartDiseaseorAttack').count()\n",
    "\n",
    "# Show the counts\n",
    "counts_heartDisBal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b1937",
   "metadata": {},
   "source": [
    "## 4.1 Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f282453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeartDiseaseorAttack: ['0', '1']\n",
      "HighBP: ['0', '1']\n",
      "HighChol: ['0', '1']\n",
      "CholCheck: ['0', '1']\n",
      "BMI: [0]\n",
      "Smoker: ['0', '1']\n",
      "Stroke: ['0', '1']\n",
      "Diabetes: [0]\n",
      "PhysActivity: ['0', '1']\n",
      "Fruits: ['0', '1']\n",
      "Veggies: ['0', '1']\n",
      "HvyAlcoholConsump: ['0', '1']\n",
      "AnyHealthcare: ['0', '1']\n",
      "NoDocDueToCost: ['0', '1']\n",
      "GenHlth: [0]\n",
      "MentHlth: [0]\n",
      "PhysHlth: [0]\n",
      "DifficultyWalking: ['0', '1']\n",
      "Sex: [0]\n",
      "Age_category: ['7', '11', '3', '8', '5', '6', '9', '1', '10', '4', '12', '13', '2']\n"
     ]
    }
   ],
   "source": [
    "#converting text into integers\n",
    "from pyspark.sql.functions import lit\n",
    "BMI_to_int = {'Underweight': 0, 'Healthy': 1, 'Overweight': 2, 'Obese': 3}\n",
    "hlth_to_int = {'Good': 1, 'Normal': 2, 'Poor': 3}\n",
    "diab_to_int = {'Type 1': 1, 'Type 2': 2}\n",
    "sex_to_int = {'Male': 1, 'Female': 0}\n",
    "\n",
    "dfBalanced = dfBalanced.withColumn('BMI', lit(0))\n",
    "for category, mapping in BMI_to_int.items():\n",
    "    dfBalanced = dfBalanced.withColumn('BMI', when(dfBalanced['BMI'] == category, mapping).otherwise(dfBalanced['BMI']))\n",
    "for col in ['GenHlth', 'PhysHlth', 'MentHlth']:\n",
    "    dfBalanced = dfBalanced.withColumn(col, lit(0))\n",
    "    for category, mapping in hlth_to_int.items():\n",
    "        dfBalanced = dfBalanced.withColumn(col, when(dfBalanced[col] == category, mapping).otherwise(dfBalanced[col]))\n",
    "dfBalanced = dfBalanced.withColumn('Diabetes', lit(0))\n",
    "for category, mapping in diab_to_int.items():\n",
    "    dfBalanced = dfBalanced.withColumn('Diabetes', when(dfBalanced['Diabetes'] == category, mapping).otherwise(dfBalanced['Diabetes']))\n",
    "dfBalanced = dfBalanced.withColumn('Sex', lit(0))\n",
    "for category, mapping in sex_to_int.items():\n",
    "    dfBalanced = dfBalanced.withColumn('Sex', when(dfBalanced['Sex'] == category, mapping).otherwise(dfBalanced['Sex']))\n",
    "    \n",
    "for column in dfBalanced.columns:\n",
    "    unique_values = dfBalanced.select(column).distinct().rdd.map(lambda x: x[0]).collect()\n",
    "    print(f\"{column}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8439faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- HeartDiseaseorAttack: integer (nullable = true)\n",
      " |-- HighBP: integer (nullable = true)\n",
      " |-- HighChol: integer (nullable = true)\n",
      " |-- CholCheck: integer (nullable = true)\n",
      " |-- BMI: integer (nullable = false)\n",
      " |-- Smoker: integer (nullable = true)\n",
      " |-- Stroke: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = false)\n",
      " |-- PhysActivity: integer (nullable = true)\n",
      " |-- Fruits: integer (nullable = true)\n",
      " |-- Veggies: integer (nullable = true)\n",
      " |-- HvyAlcoholConsump: integer (nullable = true)\n",
      " |-- AnyHealthcare: integer (nullable = true)\n",
      " |-- NoDocDueToCost: integer (nullable = true)\n",
      " |-- GenHlth: integer (nullable = false)\n",
      " |-- MentHlth: integer (nullable = false)\n",
      " |-- PhysHlth: integer (nullable = false)\n",
      " |-- DifficultyWalking: integer (nullable = true)\n",
      " |-- Sex: integer (nullable = false)\n",
      " |-- Age_category: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#converting column data type to integer\n",
    "from pyspark.sql.types import IntegerType\n",
    "columns = dfBalanced.columns\n",
    "\n",
    "for col_name in columns:\n",
    "    dfBalanced = dfBalanced.withColumn(col_name, dfBalanced[col_name].cast(IntegerType()))\n",
    "\n",
    "dfBalanced.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5be04fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 409:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_category: 0.3228888799976742\n",
      "HighBP: 0.2667256909033507\n",
      "HighChol: 0.14207949056344218\n",
      "DifficultyWalking: 0.12309167508149164\n",
      "Stroke: 0.08185453089632543\n",
      "Smoker: 0.037070451116661796\n",
      "PhysActivity: 0.01842048349811108\n",
      "CholCheck: 0.002635246977011755\n",
      "Veggies: 0.002594377007445043\n",
      "HvyAlcoholConsump: 0.0011927615695690346\n",
      "AnyHealthcare: 0.0007639948369861821\n",
      "NoDocDueToCost: 0.0005237478447310581\n",
      "Fruits: 0.00015866970719997\n",
      "BMI: 0.0\n",
      "Diabetes: 0.0\n",
      "GenHlth: 0.0\n",
      "MentHlth: 0.0\n",
      "PhysHlth: 0.0\n",
      "Sex: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 410:======================================>                  (2 + 1) / 3]\r"
     ]
    }
   ],
   "source": [
    "#feature selection algorithm\n",
    "df5 = dfBalanced\n",
    "df5 = df5.drop(\"features\")\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "features = [column for column in df5.columns if column != 'HeartDiseaseorAttack']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "df5 = assembler.transform(df5)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"HeartDiseaseorAttack\", numTrees=100)\n",
    "model = rf.fit(df5)\n",
    "feature_importance = model.featureImportances.toArray()\n",
    "feature_names = assembler.getInputCols()\n",
    "feature_importance_list = list(zip(feature_names, feature_importance))\n",
    "sorted_features = sorted(feature_importance_list, key=lambda x: -x[1])\n",
    "\n",
    "for feature, importance in sorted_features:\n",
    "    print(f'{feature}: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3940581f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HeartDiseaseorAttack',\n",
       " 'HighBP',\n",
       " 'HighChol',\n",
       " 'CholCheck',\n",
       " 'BMI',\n",
       " 'Smoker',\n",
       " 'Stroke',\n",
       " 'Diabetes',\n",
       " 'PhysActivity',\n",
       " 'Fruits',\n",
       " 'Veggies',\n",
       " 'HvyAlcoholConsump',\n",
       " 'AnyHealthcare',\n",
       " 'NoDocDueToCost',\n",
       " 'DifficultyWalking',\n",
       " 'Age_category']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping the columns\n",
    "columns_to_drop = [\"GenHlth\", \"MentHlth\", \"PhysHlth\", \"Sex\", 'features']\n",
    "\n",
    "dfFinal = dfBalanced.drop(*columns_to_drop)\n",
    "\n",
    "dfFinal.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f34ae",
   "metadata": {},
   "source": [
    "## 6.1 Data Mining Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741c23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#preparing data\n",
    "df6 = dfFinal\n",
    "df6 = df6.drop('features')\n",
    "\n",
    "#removing target column\n",
    "features = [column for column in df6.columns if column != 'HeartDiseaseorAttack']\n",
    "\n",
    "#assemble features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "df6 = assembler.transform(df6)\n",
    "\n",
    "#split the data into training and testing sets\n",
    "(training_data, testing_data) = df6.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "234db17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/11 22:54:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 443:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Accuracy: 0.7300523560209424\n",
      "Feature Rankings - \n",
      "HighBP: 0.4874402760429439\n",
      "Age_category: 0.28294937976553364\n",
      "DifficultyWalking: 0.10994913145098736\n",
      "HighChol: 0.08643806568315077\n",
      "Stroke: 0.02450703082009234\n",
      "Smoker: 0.008716116237292173\n",
      "CholCheck: 0.0\n",
      "BMI: 0.0\n",
      "Diabetes: 0.0\n",
      "PhysActivity: 0.0\n",
      "Fruits: 0.0\n",
      "Veggies: 0.0\n",
      "HvyAlcoholConsump: 0.0\n",
      "AnyHealthcare: 0.0\n",
      "NoDocDueToCost: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#gini tree\n",
    "dt = DecisionTreeClassifier(labelCol=\"HeartDiseaseorAttack\", featuresCol=\"features\", impurity='gini')\n",
    "dt_model = dt.fit(training_data)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = dt_model.transform(testing_data)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseorAttack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Gini Accuracy:\", accuracy)\n",
    "\n",
    "#feature importance\n",
    "feature_importance = dt_model.featureImportances.toArray()\n",
    "feature_rankings_table = [(feature, importance) for feature, importance in zip(features, feature_importance)]\n",
    "feature_rankings_table.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Rankings - \")\n",
    "for feature, importance in feature_rankings_table:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "424a0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy Accuracy: 0.7294240837696335\n",
      "Feature Rankings - \n",
      "HighBP: 0.4461849721668044\n",
      "Age_category: 0.31655760007438205\n",
      "DifficultyWalking: 0.12641306941977312\n",
      "HighChol: 0.0837019153518395\n",
      "Stroke: 0.01929887178471421\n",
      "Smoker: 0.007843571202486762\n",
      "CholCheck: 0.0\n",
      "BMI: 0.0\n",
      "Diabetes: 0.0\n",
      "PhysActivity: 0.0\n",
      "Fruits: 0.0\n",
      "Veggies: 0.0\n",
      "HvyAlcoholConsump: 0.0\n",
      "AnyHealthcare: 0.0\n",
      "NoDocDueToCost: 0.0\n"
     ]
    }
   ],
   "source": [
    "#entropy tree\n",
    "dt = DecisionTreeClassifier(labelCol=\"HeartDiseaseorAttack\", featuresCol=\"features\", impurity='entropy')\n",
    "dt_model = dt.fit(training_data)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = dt_model.transform(testing_data)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseorAttack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Entropy Accuracy:\", accuracy)\n",
    "\n",
    "#feature importance\n",
    "feature_importance = dt_model.featureImportances.toArray()\n",
    "feature_rankings_table = [(feature, importance) for feature, importance in zip(features, feature_importance)]\n",
    "feature_rankings_table.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Rankings - \")\n",
    "for feature, importance in feature_rankings_table:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7794ea99",
   "metadata": {},
   "source": [
    "## 6.3 Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1625934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tuning max depth\n",
    "# for i in range(3, 11):\n",
    "#     dt = DecisionTreeClassifier(labelCol=\"HeartDiseaseorAttack\", featuresCol=\"features\", impurity='gini', maxDepth=i)\n",
    "#     dt_model = dt.fit(training_data)\n",
    "#     predictions = dt_model.transform(testing_data)\n",
    "#     evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseorAttack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#     accuracy = evaluator.evaluate(predictions)\n",
    "#     print(\"Accuracy with max_depth:\", i, \": \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f223bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minInstancesPerNode: 5\n",
      "minInfoGain: 0.0\n"
     ]
    }
   ],
   "source": [
    "#tuning Min Instances Per Node and Min Info Gain\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='HeartDiseaseorAttack', impurity='gini', maxDepth=6)\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.minInstancesPerNode, [5, 10, 15]) \\\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.1, 0.2]) \\\n",
    "    .build()\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "best_model = None\n",
    "best_auc = 0.0\n",
    "\n",
    "for params in param_grid:\n",
    "    model = pipeline.fit(training_data, params=params)\n",
    "    predictions = model.transform(testing_data)\n",
    "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\", evaluator.labelCol: \"HeartDiseaseorAttack\"})\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_model = model\n",
    "        \n",
    "best_params = best_model.stages[-1].extractParamMap()\n",
    "min_instances_per_node = best_params[dt.minInstancesPerNode]\n",
    "min_info_gain = best_params[dt.minInfoGain]\n",
    "\n",
    "print(\"minInstancesPerNode:\", min_instances_per_node)\n",
    "print(\"minInfoGain:\", min_info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9694d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 836:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model Accuracy: 0.7389528795811519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#testing tuned parameters\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='HeartDiseaseorAttack', \n",
    "                            impurity='gini', maxDepth=6, minInstancesPerNode=5, minInfoGain=0.0)\n",
    "dt_model = dt.fit(training_data)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = dt_model.transform(testing_data)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseorAttack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Final model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173dc90",
   "metadata": {},
   "source": [
    "## 7.2.1 Final Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61ce6ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model Accuracy: 0.7390575916230366\n"
     ]
    }
   ],
   "source": [
    "#building final model\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='HeartDiseaseorAttack', impurity='gini', maxDepth=6)\n",
    "dt_final = dt.fit(training_data)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = dt_final.transform(testing_data)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"HeartDiseaseorAttack\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Final model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dda6dd85",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3760487706.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [44]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install sklearn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/11 23:19:40 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-30d6bfa4-b828-413c-8336-04bbd983d936. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-30d6bfa4-b828-413c-8336-04bbd983d936\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1164)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:318)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:314)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:314)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:309)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1989)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:92)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$38(SparkContext.scala:667)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55192)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "!pip install sklearn\n",
    "from sklearn import tree\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(dt_final, feature_names=training_data.columns, class_names=str(dt_final.classes_), filled=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
